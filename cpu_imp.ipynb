{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a73957",
   "metadata": {},
   "source": [
    "# todo'lar icin arastirmalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43140622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "tags_hashing_tf = HashingTF(inputCol=\"tags_list\", outputCol=\"tags_raw_features\", numFeatures=5000)\n",
    "tags_idf = IDF(inputCol=\"tags_raw_features\", outputCol=\"tags_features\")\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"text_features\", \"tags_features\", \"title_len\", \"body_len\"], outputCol=\"features\")\n",
    "lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, tags_hashing_tf, tags_idf,\n",
    "feature_assembler, lr])\n",
    "df_clean = df.na.drop(subset=[\"Title\", \"Body\", \"Tags\", \"Y\"])\n",
    "df_clean = df_clean.withColumn(\"tags_list\", spark_split(regexp_replace(col(\"Tags\"), \"[<>]\", \" \"), \" \"))\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ngram = NGram(n=2, inputCol=\"filtered_words\", outputCol=\"bigrams\")\n",
    "hashing_tf = HashingTF(inputCols=[\"filtered_words\", \"bigrams\"], outputCol=\"raw_features\", numFeatures=20000)\n",
    "lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, ngram, hashing_tf, idf, feature_assembler, lr])\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "word2vec = Word2Vec(vectorSize=100, minCount=5, inputCol=\"filtered_words\", outputCol=\"w2v_features\")\n",
    "# vector size kelime basina temsil vektoru(HP), mincount modele eklenecek kelimenin min frekansi\n",
    "feature_assembler = VectorAssembler(inputCols=[\"w2v_features\", \"title_len\", \"body_len\"],outputCol=\"features\")\n",
    "lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, word2vec, feature_assembler, lr])\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "# hp opt icin: pyspark'ta crossvalidator ve paramgridbuilder\n",
    "pipeline_for_tuning = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler])\n",
    "rf_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, rf])\n",
    "rf.setSeed(42); paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [50, 100, 150]).addGrid(rf.maxDepth, [5, 10, 20]).build()\n",
    "crossval = CrossValidator(estimator=rf_pipeline, estimatorParamMaps=paramGrid,\n",
    "evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"),\n",
    "numFolds=3); cv_model = crossval.fit(train_data); rf_predictions = cv_model.transform(test_data);\n",
    "best_model = cv_model.bestModel #en iyi parametreler:best_model.stages[-1].getNumTrees() ve getMaxDepth()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653497c5",
   "metadata": {},
   "source": [
    "1. import'lar ve udf'ler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession; import re; import traceback;\n",
    "from pyspark.sql.functions import udf, col, length, concat_ws \n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler, NGram\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def count_punctuation(text):\n",
    "    if text is None: return 0\n",
    "    return len(re.findall(r'[?!]', text))\n",
    "\n",
    "def avg_word_length(text):\n",
    "    if text is None: return 0.0\n",
    "    words = text.split()\n",
    "    if not words: return 0.0\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    if text is None: return None\n",
    "    return re.sub(re.compile('<.*?>'), '', text)\n",
    "\n",
    "def clean_tags(tags):\n",
    "    if tags is None: return []\n",
    "    return tags.replace('<', ' ').replace('>', ' ').strip().split()\n",
    "\n",
    "count_punct_udf = udf(count_punctuation, IntegerType())\n",
    "avg_word_len_udf = udf(avg_word_length, DoubleType())\n",
    "remove_html_udf = udf(remove_html_tags, StringType())\n",
    "clean_tags_udf = udf(clean_tags, ArrayType()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c8e86",
   "metadata": {},
   "source": [
    "2. main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f9103",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CPU_IMP\").config(\"spark.driver.memory\", \"8g\").getOrCreate() \n",
    "data_path = \"data/train.csv\"\n",
    "\n",
    "try:\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(data_path)\n",
    "    df.printSchema()\n",
    "    df.groupBy(\"Y\").count().show()\n",
    "    initial_count = df.count()\n",
    "    df_clean = df.na.drop(subset=[\"Title\", \"Body\", \"Y\"]).withColumn(\"CleanBody\", remove_html_udf(col(\"Body\")))\\\n",
    "        .withColumn(\"text\", concat_ws(\" \", col(\"Title\"), col(\"CleanBody\")))\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol=\"Y\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    \n",
    "    hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20000)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"text_features\")\n",
    "    \n",
    "    df_featured = df_clean.withColumn(\"title_len\", length(col(\"Title\"))) \\\n",
    "        .withColumn(\"body_len\", length(col(\"CleanBody\"))) \\\n",
    "        .withColumn(\"punct_count\", count_punct_udf(col(\"text\"))) \\\n",
    "        .withColumn(\"avg_word_len\", avg_word_len_udf(col(\"text\")))\n",
    "\n",
    "    feature_assembler = VectorAssembler(\n",
    "        inputCols=[\"text_features\", \"title_len\", \"body_len\", \"punct_count\", \"avg_word_len\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    (train_data, test_data) = df_featured.randomSplit([0.8, 0.2], seed=42)\n",
    "    train_data.cache(); test_data.cache()\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "    lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, lr])\n",
    "    lr_model = lr_pipeline.fit(train_data)\n",
    "    lr_predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    accuracy = evaluator.setMetricName(\"accuracy\").evaluate(lr_predictions)\n",
    "    f1_score = evaluator.setMetricName(\"f1\").evaluate(lr_predictions)\n",
    "    \n",
    "    print(\"\\nLogistic Regression Evaluation\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    lr_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "    rf_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, rf])\n",
    "    \n",
    "    rf_model = rf_pipeline.fit(train_data)\n",
    "    rf_predictions = rf_model.transform(test_data)\n",
    "    accuracy = evaluator.setMetricName(\"accuracy\").evaluate(rf_predictions)\n",
    "    f1_score = evaluator.setMetricName(\"f1\").evaluate(rf_predictions)\n",
    "\n",
    "    print(\"\\nRandom Forest Evaluation\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    rf_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"{e}\")\n",
    "    traceback.print_exc()\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
