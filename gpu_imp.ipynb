{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b1e665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/20 23:01:55 WARN Utils: Your hostname, DESKTOP-15VE119 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/20 23:01:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/20 23:01:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/20 23:01:57 WARN RapidsPluginUtils: RAPIDS Accelerator 24.02.0 using cudf 24.02.1.\n",
      "25/07/20 23:01:58 WARN RapidsPluginUtils: Multiple cudf jars found in the classpath:\n",
      "revison: dd34fdbe35e68ba56a2183f11ed822ddaa6c927b\n",
      "\tjar URL: jar:file:/home/bunveren/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/pyspark/jars/rapids-4-spark_2.12-24.02.0.jar\n",
      "\tversion=24.02.1\n",
      "\tuser=\n",
      "\trevision=dd34fdbe35e68ba56a2183f11ed822ddaa6c927b\n",
      "\tbranch=HEAD\n",
      "\tdate=2024-02-28T05:34:16Z\n",
      "\turl=https://github.com/rapidsai/cudf.git\n",
      "\tjar URL: jar:file:/home/bunveren/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/pyspark/jars/cudf-24.02.2-cuda12.jar\n",
      "\tversion=24.02.2\n",
      "\tuser=\n",
      "\trevision=dd34fdbe35e68ba56a2183f11ed822ddaa6c927b\n",
      "\tbranch=HEAD\n",
      "\tdate=2024-02-28T07:51:45Z\n",
      "Please make sure there is only one cudf jar in the classpath. If it is impossible to fix the classpath you can suppress the error by setting spark.rapids.sql.allowMultipleJars to ALWAYS, but this can cause unpredictable behavior as the plugin may pick up the wrong jar.\n",
      "25/07/20 23:01:58 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set `spark.rapids.sql.enabled` to false.\n",
      "25/07/20 23:01:58 WARN RapidsPluginUtils: spark.rapids.sql.explain is set to `NOT_ON_GPU`. Set it to 'NONE' to suppress the diagnostics logging about the query placement on the GPU.\n",
      "25/07/20 23:02:08 WARN RapidsPluginUtils: Multiple cudf jars found in the classpath:\n",
      "revison: dd34fdbe35e68ba56a2183f11ed822ddaa6c927b\n",
      "\tjar URL: jar:file:/home/bunveren/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/pyspark/jars/rapids-4-spark_2.12-24.02.0.jar\n",
      "\tversion=24.02.1\n",
      "\tuser=\n",
      "\trevision=dd34fdbe35e68ba56a2183f11ed822ddaa6c927b\n",
      "\tbranch=HEAD\n",
      "\tdate=2024-02-28T05:34:16Z\n",
      "\turl=https://github.com/rapidsai/cudf.git\n",
      "\tjar URL: jar:file:/home/bunveren/miniconda3/envs/rapids-24.02/lib/python3.10/site-packages/pyspark/jars/cudf-24.02.2-cuda12.jar\n",
      "\tversion=24.02.2\n",
      "\tuser=\n",
      "\trevision=dd34fdbe35e68ba56a2183f11ed822ddaa6c927b\n",
      "\tbranch=HEAD\n",
      "\tdate=2024-02-28T07:51:45Z\n",
      "Please make sure there is only one cudf jar in the classpath. If it is impossible to fix the classpath you can suppress the error by setting spark.rapids.sql.allowMultipleJars to ALWAYS, but this can cause unpredictable behavior as the plugin may pick up the wrong jar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session oluşturuldu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/20 23:02:10 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basit bir DataFrame oluşturuldu. Şimdi count() işlemi başlıyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/20 23:02:17 WARN GpuOverrides: \n",
      "        ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "          @Expression <AttributeReference> language#0 could run on GPU\n",
      "          @Expression <AttributeReference> users_count#1L could run on GPU\n",
      "\n",
      "25/07/20 23:02:18 WARN GpuOverrides: \n",
      "        ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "          @Expression <AttributeReference> language#0 could run on GPU\n",
      "          @Expression <AttributeReference> users_count#1L could run on GPU\n",
      "\n",
      "25/07/20 23:02:18 WARN GpuOverrides: \n",
      "        ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "          @Expression <AttributeReference> language#0 could run on GPU\n",
      "          @Expression <AttributeReference> users_count#1L could run on GPU\n",
      "\n",
      "25/07/20 23:02:18 WARN GpuOverrides: \n",
      "      ! <RDDScanExec> cannot run on GPU because GPU does not currently support the operator class org.apache.spark.sql.execution.RDDScanExec\n",
      "        @Expression <AttributeReference> language#0 could run on GPU\n",
      "        @Expression <AttributeReference> users_count#1L could run on GPU\n",
      "\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAŞARILI! DataFrame'de 3 satır var.\n",
      "Temel Spark-GPU entegrasyonu çalışıyor. Sorun pipeline'ın ileriki adımlarında.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport traceback\\nfrom pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, concat_ws, length, regexp_replace, size, split\\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\\nfrom pyspark.ml import Pipeline\\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\\n\\ndef main():\\n    spark = SparkSession.builder         .appName(\"GPU_IMP_Optimized\")         .master(\"local[*]\")         .config(\"spark.driver.memory\", \"8g\")         .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")         .config(\"spark.sql.files.maxPartitionBytes\", \"512m\")         .config(\"spark.rapids.sql.explain\", \"ALL\")         .config(\"spark.rapids.ml.enabled\", \"true\")         .config(\"spark.sql.session.timeZone\", \"UTC\")         .getOrCreate()\\n        \\n    data_path = \"/mnt/c/Users/BerenÜnveren/Desktop/BIL401/data/train.csv\"\\n\\n    schema = StructType([\\n        StructField(\"Id\", IntegerType(), True),\\n        StructField(\"Title\", StringType(), True),\\n        StructField(\"Body\", StringType(), True),\\n        StructField(\"Y\", StringType(), True)\\n    ])\\n\\n    df = spark.read.format(\"csv\")         .schema(schema)         .option(\"header\", \"true\")         .option(\"quote\", \"\"\")         .option(\"multiLine\", \"true\")         .load(data_path)\\n    \\n    df.printSchema()\\n    df.groupBy(\"Y\").count().show()\\n\\n    df_clean = df.na.drop(subset=[\"Title\", \"Body\", \"Y\"])         .withColumn(\"CleanBody\", regexp_replace(col(\"Body\"), \"<.*?>\", \"\"))         .withColumn(\"text\", concat_ws(\" \", col(\"Title\"), col(\"CleanBody\")))\\n\\n    df_featured = df_clean.withColumn(\"title_len\", length(col(\"Title\")))         .withColumn(\"body_len\", length(col(\"CleanBody\")))         .withColumn(\"punct_count\", length(col(\"text\")) - length(regexp_replace(col(\"text\"), \"[?!]\", \"\")))         .withColumn(\"avg_word_len\", length(regexp_replace(col(\"text\"), \" \", \"\")) / (size(split(col(\"text\"), \" \")) + 1e-6))\\n    \\n    label_indexer = StringIndexer(inputCol=\"Y\", outputCol=\"label\", handleInvalid=\"skip\")\\n    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\\n    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\\n    hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20000)\\n    idf = IDF(inputCol=\"raw_features\", outputCol=\"text_features\")\\n    \\n    feature_assembler = VectorAssembler(\\n        inputCols=[\"text_features\", \"title_len\", \"body_len\", \"punct_count\", \"avg_word_len\"],\\n        outputCol=\"features\"\\n    )\\n\\n    (train_data, test_data) = df_featured.randomSplit([0.8, 0.2], seed=42)\\n    train_data.cache()\\n    test_data.cache()\\n    \\n    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\\n    lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, lr])\\n    lr_model = lr_pipeline.fit(train_data)\\n    lr_predictions = lr_model.transform(test_data)\\n    \\n    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\\n    lr_accuracy = evaluator.setMetricName(\"accuracy\").evaluate(lr_predictions)\\n    lr_f1 = evaluator.setMetricName(\"f1\").evaluate(lr_predictions)\\n    \\n    print(\"\\nLogistic Regression Evaluation\")\\n    print(f\"Accuracy: {lr_accuracy:.4f}\")\\n    print(f\"F1 Score: {lr_f1:.4f}\")\\n    print(\"Confusion Matrix:\")\\n    lr_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\\n\\n    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\\n    rf_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, rf])\\n    \\n    rf_model = rf_pipeline.fit(train_data)\\n    rf_predictions = rf_model.transform(test_data)\\n    rf_accuracy = evaluator.setMetricName(\"accuracy\").evaluate(rf_predictions)\\n    rf_f1 = evaluator.setMetricName(\"f1\").evaluate(rf_predictions)\\n\\n    print(\"\\nRandom Forest Evaluation\")\\n    print(f\"Accuracy: {rf_accuracy:.4f}\")\\n    print(f\"F1 Score: {rf_f1:.4f}\")\\n    print(\"Confusion Matrix:\")\\n    rf_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\\n\\nif __name__ == \\'__main__\\':\\n    try:\\n        main()\\n    except Exception as e:\\n        print(f\"An error occurred: {e}\")\\n        traceback.print_exc()\\n    finally:\\n        from pyspark.sql import SparkSession\\n        spark = SparkSession.getActiveSession()\\n        if spark:\\n            spark.stop()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GPU_FINAL_TEST\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session oluşturuldu.\")\n",
    "\n",
    "data = [(\"Java\", 20000), (\"Python\", 100000), (\"Scala\", 3000)]\n",
    "columns = [\"language\", \"users_count\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Basit bir DataFrame oluşturuldu. Şimdi count() işlemi başlıyor...\")\n",
    "\n",
    "try:\n",
    "    record_count = df.count()\n",
    "    print(f\"Başarılı, DataFrame'de {record_count} satır var.\")\n",
    "    print(\"Temel Spark-GPU entegrasyonu çalışıyor. Sorun pipeline'ın ileriki adımlarında.\")\n",
    "except Exception as e:\n",
    "    print(f\"HATA! count() işlemi sırasında bir sorun oluştu: {e}\")\n",
    "    print(\"Temel Spark-GPU entegrasyonu çalışmıyor. Sorun ortamın kendisinde.\")\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import traceback\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, length, regexp_replace, size, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"GPU_IMP_Optimized\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"512m\") \\\n",
    "        .config(\"spark.rapids.sql.explain\", \"ALL\") \\\n",
    "        .config(\"spark.rapids.ml.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    data_path = \"/mnt/c/Users/BerenÜnveren/Desktop/BIL401/data/train.csv\"\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"Id\", IntegerType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Body\", StringType(), True),\n",
    "        StructField(\"Y\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .load(data_path)\n",
    "    \n",
    "    df.printSchema()\n",
    "    df.groupBy(\"Y\").count().show()\n",
    "\n",
    "    df_clean = df.na.drop(subset=[\"Title\", \"Body\", \"Y\"]) \\\n",
    "        .withColumn(\"CleanBody\", regexp_replace(col(\"Body\"), \"<.*?>\", \"\")) \\\n",
    "        .withColumn(\"text\", concat_ws(\" \", col(\"Title\"), col(\"CleanBody\")))\n",
    "\n",
    "    df_featured = df_clean.withColumn(\"title_len\", length(col(\"Title\"))) \\\n",
    "        .withColumn(\"body_len\", length(col(\"CleanBody\"))) \\\n",
    "        .withColumn(\"punct_count\", length(col(\"text\")) - length(regexp_replace(col(\"text\"), \"[?!]\", \"\"))) \\\n",
    "        .withColumn(\"avg_word_len\", length(regexp_replace(col(\"text\"), \" \", \"\")) / (size(split(col(\"text\"), \" \")) + 1e-6))\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol=\"Y\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20000)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"text_features\")\n",
    "    \n",
    "    feature_assembler = VectorAssembler(\n",
    "        inputCols=[\"text_features\", \"title_len\", \"body_len\", \"punct_count\", \"avg_word_len\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    (train_data, test_data) = df_featured.randomSplit([0.8, 0.2], seed=42)\n",
    "    train_data.cache()\n",
    "    test_data.cache()\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "    lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, lr])\n",
    "    lr_model = lr_pipeline.fit(train_data)\n",
    "    lr_predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    lr_accuracy = evaluator.setMetricName(\"accuracy\").evaluate(lr_predictions)\n",
    "    lr_f1 = evaluator.setMetricName(\"f1\").evaluate(lr_predictions)\n",
    "    \n",
    "    print(\"\\nLogistic Regression Evaluation\")\n",
    "    print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {lr_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    lr_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "    rf_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, rf])\n",
    "    \n",
    "    rf_model = rf_pipeline.fit(train_data)\n",
    "    rf_predictions = rf_model.transform(test_data)\n",
    "    rf_accuracy = evaluator.setMetricName(\"accuracy\").evaluate(rf_predictions)\n",
    "    rf_f1 = evaluator.setMetricName(\"f1\").evaluate(rf_predictions)\n",
    "\n",
    "    print(\"\\nRandom Forest Evaluation\")\n",
    "    print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {rf_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    rf_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            spark.stop()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83c513-5b39-4f43-857e-68d4f52d46d4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAPIDS 24.02)",
   "language": "python",
   "name": "rapids-24.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
