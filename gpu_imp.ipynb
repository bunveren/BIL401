{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b1e665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- Y: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/20 22:46:13 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <HashAggregateExec> will run on GPU\n",
      "    *Expression <AggregateExpression> count(1) will run on GPU\n",
      "      *Expression <Count> count(1) will run on GPU\n",
      "    *Expression <Alias> toprettystring(Y#40, Some(UTC)) AS toprettystring(Y)#56 will run on GPU\n",
      "      *Expression <ToPrettyString> toprettystring(Y#40, Some(UTC)) will run on GPU\n",
      "    *Expression <Alias> toprettystring(count(1)#49L, Some(UTC)) AS toprettystring(count)#57 will run on GPU\n",
      "      *Expression <ToPrettyString> toprettystring(count(1)#49L, Some(UTC)) will run on GPU\n",
      "    *Exec <ShuffleExchangeExec> will run on GPU\n",
      "      *Partitioning <HashPartitioning> will run on GPU\n",
      "      *Exec <HashAggregateExec> will run on GPU\n",
      "        *Expression <AggregateExpression> partial_count(1) will run on GPU\n",
      "          *Expression <Count> count(1) will run on GPU\n",
      "        *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "25/07/20 22:46:13 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <HashAggregateExec> will run on GPU\n",
      "    *Expression <AggregateExpression> count(1) will run on GPU\n",
      "      *Expression <Count> count(1) will run on GPU\n",
      "    *Expression <Alias> toprettystring(Y#40, Some(UTC)) AS toprettystring(Y)#56 will run on GPU\n",
      "      *Expression <ToPrettyString> toprettystring(Y#40, Some(UTC)) will run on GPU\n",
      "    *Expression <Alias> toprettystring(count(1)#49L, Some(UTC)) AS toprettystring(count)#57 will run on GPU\n",
      "      *Expression <ToPrettyString> toprettystring(count(1)#49L, Some(UTC)) will run on GPU\n",
      "    *Exec <ShuffleExchangeExec> will run on GPU\n",
      "      *Partitioning <HashPartitioning> will run on GPU\n",
      "      *Exec <HashAggregateExec> will run on GPU\n",
      "        *Expression <AggregateExpression> partial_count(1) will run on GPU\n",
      "          *Expression <Count> count(1) will run on GPU\n",
      "        *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "25/07/20 22:46:13 WARN GpuOverrides: \n",
      "!Exec <CollectLimitExec> cannot run on GPU because the Exec CollectLimitExec has been disabled, and is disabled by default because Collect Limit replacement can be slower on the GPU, if huge number of rows in a batch it could help by limiting the number of rows transferred from GPU to CPU. Set spark.rapids.sql.exec.CollectLimitExec to true if you wish to enable it\n",
      "  @Partitioning <SinglePartition$> could run on GPU\n",
      "  *Exec <HashAggregateExec> will run on GPU\n",
      "    *Expression <AggregateExpression> count(1) will run on GPU\n",
      "      *Expression <Count> count(1) will run on GPU\n",
      "    *Expression <Alias> toprettystring(Y#40, Some(UTC)) AS toprettystring(Y)#56 will run on GPU\n",
      "      *Expression <ToPrettyString> toprettystring(Y#40, Some(UTC)) will run on GPU\n",
      "    *Expression <Alias> toprettystring(count(1)#49L, Some(UTC)) AS toprettystring(count)#57 will run on GPU\n",
      "      *Expression <ToPrettyString> toprettystring(count(1)#49L, Some(UTC)) will run on GPU\n",
      "    *Exec <ShuffleExchangeExec> will run on GPU\n",
      "      *Partitioning <HashPartitioning> will run on GPU\n",
      "      *Exec <HashAggregateExec> will run on GPU\n",
      "        *Expression <AggregateExpression> partial_count(1) will run on GPU\n",
      "          *Expression <Count> count(1) will run on GPU\n",
      "        *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "25/07/20 22:46:13 WARN GpuOverrides: \n",
      "*Exec <ShuffleExchangeExec> will run on GPU\n",
      "  *Partitioning <HashPartitioning> will run on GPU\n",
      "  *Exec <HashAggregateExec> will run on GPU\n",
      "    *Expression <AggregateExpression> partial_count(1) will run on GPU\n",
      "      *Expression <Count> count(1) will run on GPU\n",
      "    *Exec <FileSourceScanExec> will run on GPU\n",
      "\n",
      "[Stage 0:>                  (0 + 0) / 1][Stage 1:>                  (0 + 0) / 1]"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat_ws, length, regexp_replace, size, split\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"GPU_IMP_Optimized\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "        .config(\"spark.sql.files.maxPartitionBytes\", \"512m\") \\\n",
    "        .config(\"spark.executor.resource.gpu.amount\", \"1\") \\\n",
    "        .config(\"spark.task.resource.gpu.amount\", \"1\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.rapids.sql.explain\", \"ALL\") \\\n",
    "        .config(\"spark.rapids.ml.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\") \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    data_path = \"/mnt/c/Users/Beren√únveren/Desktop/BIL401/data/train.csv\"\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"Id\", IntegerType(), True),\n",
    "        StructField(\"Title\", StringType(), True),\n",
    "        StructField(\"Body\", StringType(), True),\n",
    "        StructField(\"Y\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .load(data_path)\n",
    "    \n",
    "    df.printSchema()\n",
    "    df.groupBy(\"Y\").count().show()\n",
    "\n",
    "    df_clean = df.na.drop(subset=[\"Title\", \"Body\", \"Y\"]) \\\n",
    "        .withColumn(\"CleanBody\", regexp_replace(col(\"Body\"), \"<.*?>\", \"\")) \\\n",
    "        .withColumn(\"text\", concat_ws(\" \", col(\"Title\"), col(\"CleanBody\")))\n",
    "\n",
    "    df_featured = df_clean.withColumn(\"title_len\", length(col(\"Title\"))) \\\n",
    "        .withColumn(\"body_len\", length(col(\"CleanBody\"))) \\\n",
    "        .withColumn(\"punct_count\", length(col(\"text\")) - length(regexp_replace(col(\"text\"), \"[?!]\", \"\"))) \\\n",
    "        .withColumn(\"avg_word_len\", length(regexp_replace(col(\"text\"), \" \", \"\")) / (size(split(col(\"text\"), \" \")) + 1e-6))\n",
    "    \n",
    "    label_indexer = StringIndexer(inputCol=\"Y\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "    tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "    stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "    hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=20000)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"text_features\")\n",
    "    \n",
    "    feature_assembler = VectorAssembler(\n",
    "        inputCols=[\"text_features\", \"title_len\", \"body_len\", \"punct_count\", \"avg_word_len\"],\n",
    "        outputCol=\"features\"\n",
    "    )\n",
    "\n",
    "    (train_data, test_data) = df_featured.randomSplit([0.8, 0.2], seed=42)\n",
    "    train_data.cache()\n",
    "    test_data.cache()\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "    lr_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, lr])\n",
    "    lr_model = lr_pipeline.fit(train_data)\n",
    "    lr_predictions = lr_model.transform(test_data)\n",
    "    \n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    lr_accuracy = evaluator.setMetricName(\"accuracy\").evaluate(lr_predictions)\n",
    "    lr_f1 = evaluator.setMetricName(\"f1\").evaluate(lr_predictions)\n",
    "    \n",
    "    print(\"\\nLogistic Regression Evaluation\")\n",
    "    print(f\"Accuracy: {lr_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {lr_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    lr_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "    rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100)\n",
    "    rf_pipeline = Pipeline(stages=[label_indexer, tokenizer, stopwords_remover, hashing_tf, idf, feature_assembler, rf])\n",
    "    \n",
    "    rf_model = rf_pipeline.fit(train_data)\n",
    "    rf_predictions = rf_model.transform(test_data)\n",
    "    rf_accuracy = evaluator.setMetricName(\"accuracy\").evaluate(rf_predictions)\n",
    "    rf_f1 = evaluator.setMetricName(\"f1\").evaluate(rf_predictions)\n",
    "\n",
    "    print(\"\\nRandom Forest Evaluation\")\n",
    "    print(f\"Accuracy: {rf_accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {rf_f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    rf_predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        from pyspark.sql import SparkSession\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83c513-5b39-4f43-857e-68d4f52d46d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAPIDS 24.02)",
   "language": "python",
   "name": "rapids-24.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
